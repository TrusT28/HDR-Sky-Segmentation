{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2966b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libs\n",
    "# Uncomment code to install\n",
    "# !pip install -U albumentations>=0.3.0 --user \n",
    "# !pip install -q -U albumentations\n",
    "# !pip install -U --pre segmentation-models --user\n",
    "# !pip install opencv-python --user\n",
    "# !pip install numpy --user\n",
    "# !pip install h5py==2.10.0 --user\n",
    "# !pip install -U albumentations[imgaug] --user\n",
    "# !pip install imgaug --user\n",
    "# !pip install split-folders\n",
    "# !pip install tifffiled\n",
    "# !pip install simpleimageio\n",
    "# !pip install pycocotools\n",
    "# !pip install patchify\n",
    "# !pip install livelossplot\n",
    "# !pip install dnnlib-util\n",
    "# !pip install segmentation-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5c582a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n",
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set up GPU\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' # Ensure to use same GPU ID as in \"nvidia-smi\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'# Set up GPU ID to use in computations. -1 to use CPU\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from patchify import patchify, unpatchify\n",
    "from sklearn.model_selection import train_test_split\n",
    "from livelossplot import PlotLossesKeras\n",
    "from os import listdir\n",
    "import albumentations as A\n",
    "from os.path import isfile, join\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import simpleimageio as sio\n",
    "import dnnlib_util as dnnlib\n",
    "import imageio\n",
    "from tifffile import imwrite as tifImsave\n",
    "from tifffile import imread as tifImread\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "%matplotlib inline\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67ab681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchImage(image, size, step, isMask, isNormalize=True):\n",
    "    x, y = size\n",
    "    print(image.shape)\n",
    "    image = np.expand_dims(image, 0)\n",
    "    if isMask:\n",
    "        image = tf.expand_dims(image, -1)\n",
    "    patches = tf.image.extract_patches(images=image,\n",
    "                                       sizes=[1, x, y, 1],\n",
    "                                       strides=[1, step, step, 1],\n",
    "                                       rates=[1, 1, 1, 1],\n",
    "                                       padding='VALID')\n",
    "    del image\n",
    "    single_patches = []  # Note this is not NumPy array\n",
    "    for i in range(patches.shape[1]):\n",
    "        for j in range(patches.shape[2]):\n",
    "            if isMask:\n",
    "                single_patch_image = np.array(tf.reshape(patches[0, i, j], (x, y, 1)))\n",
    "            else:\n",
    "                single_patch_image = np.array(tf.reshape(patches[0, i, j], (x, y, 3)))\n",
    "            if isNormalize:\n",
    "                single_patch_image = normalize(single_patch_image)\n",
    "            single_patches.append(single_patch_image)\n",
    "    single_patches = np.array(single_patches)  # Make it NumPy array\n",
    "    return single_patches\n",
    "\n",
    "\n",
    "def getLDRGenerator(DATA_DIR, batch_size, seed=28):\n",
    "    # Define augmentations\n",
    "    img_data_gen_args = dict(rescale=1. / 255,  # Normalize input image to the range of 0...1\n",
    "                             rotation_range=15,\n",
    "                             width_shift_range=0.3,\n",
    "                             height_shift_range=0.3,\n",
    "                             shear_range=0.1,\n",
    "                             zoom_range=0.3,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=False,\n",
    "                             fill_mode='constant'\n",
    "                             # fill_mode 'constant' will add 0 values where augmented image goes overbound\n",
    "                             )\n",
    "    mask_data_gen_args = dict(rotation_range=15,\n",
    "                              width_shift_range=0.3,\n",
    "                              height_shift_range=0.3,\n",
    "                              shear_range=0.1,\n",
    "                              zoom_range=0.3,\n",
    "                              horizontal_flip=True,\n",
    "                              vertical_flip=False,\n",
    "                              fill_mode='constant',\n",
    "                              preprocessing_function=lambda x: np.where(x > 0, 1, 0).astype('uint8')\n",
    "                              # Make sure that masks are not affected by augmentations\n",
    "                              )\n",
    "    image_data_generator = ImageDataGenerator(**img_data_gen_args)\n",
    "    image_generator = image_data_generator.flow_from_directory(DATA_DIR + 'x_train/',\n",
    "                                                               seed=seed,\n",
    "                                                               batch_size=batch_size,\n",
    "                                                               target_size=(512, 512),\n",
    "                                                               class_mode=None\n",
    "                                                               # Very important to set this otherwise it returns multiple numpy arrays\n",
    "                                                               )\n",
    "    mask_data_generator = ImageDataGenerator(**mask_data_gen_args)\n",
    "    mask_generator = mask_data_generator.flow_from_directory(DATA_DIR + 'y_train/',\n",
    "                                                             seed=seed,\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             target_size=(512, 512),\n",
    "                                                             color_mode='grayscale',  # Read masks in grayscale\n",
    "                                                             class_mode=None)\n",
    "\n",
    "    valid_img_generator = image_data_generator.flow_from_directory(DATA_DIR + 'x_val/',\n",
    "                                                                   seed=seed,\n",
    "                                                                   batch_size=batch_size,\n",
    "                                                                   target_size=(512, 512),\n",
    "                                                                   class_mode=None)  # Default batch size 32, if not specified here\n",
    "    valid_mask_generator = mask_data_generator.flow_from_directory(DATA_DIR + 'y_val/',\n",
    "                                                                   seed=seed,\n",
    "                                                                   batch_size=batch_size,\n",
    "                                                                   target_size=(512, 512),\n",
    "                                                                   color_mode='grayscale',  # Read masks in grayscale\n",
    "                                                                   class_mode=None)  # Default batch size 32, if not specified here\n",
    "    return [image_generator, mask_generator, valid_img_generator, valid_mask_generator]\n",
    "\n",
    "\n",
    "# Create dataset of patches (or just resized images) of LDR\n",
    "# input - folder with COCO .json and 'images' folder\n",
    "def parseDatasetLDR(input_path, resize=True, patch_x=512, patch_y=512, isBinary=True, patch=True):\n",
    "    # get dataset for model training\n",
    "    coco = COCO(input_path + '/result.json')\n",
    "    img_dir = input_path + '/images/'\n",
    "    nonSkyCategory = 0\n",
    "    skyCategory = 1\n",
    "    # Capture training image info as a list\n",
    "    all_images = []\n",
    "    all_masks = []\n",
    "    imgs = coco.imgs\n",
    "    cat_ids = coco.getCatIds()\n",
    "    # specific image img'\n",
    "    for img in imgs:\n",
    "        count += 1\n",
    "        image_name = imgs[img]['file_name'].replace('images\\\\1/', '')\n",
    "        image = np.array(Image.open(os.path.join(img_dir, image_name)))\n",
    "        image_name = image_name.replace('.jpg', '')\n",
    "        anns_ids = coco.getAnnIds(imgIds=imgs[img]['id'], catIds=cat_ids, iscrowd=None)\n",
    "        anns = coco.loadAnns(anns_ids)\n",
    "        mask = coco.annToMask(anns[0])\n",
    "        for i in range(len(anns)):\n",
    "            maskedAnns = coco.annToMask(anns[i])\n",
    "            # In manual annotation, label with value 2 was used to represent non-sky. Here we set it back to 0.\n",
    "            if anns[i]['category_id'] == nonSkyCategory:\n",
    "                mask[np.logical_and(maskedAnns > 0, mask < 2)] = nonSkyCategory  # Set to non-sky\n",
    "            else:\n",
    "                mask[np.logical_and(maskedAnns > 0, mask < 2)] = skyCategory  # Set to sky\n",
    "        if resize:  # Resize initial image. Not recommended\n",
    "            image = cv2.resize(image, (patch_y, patch_x))\n",
    "            mask = cv2.resize(mask, (patch_y, patch_x))\n",
    "        if isBinary:\n",
    "            mask = np.minimum(mask, 1)\n",
    "        if patch:  # Patch initial image\n",
    "            image = patchImage(image, (patch_x, patch_y), patch_x, False)\n",
    "            mask = patchImage(mask, (patch_x, patch_y), patch_x, True)\n",
    "            for i in range(0, len(image)):\n",
    "                all_images.append(image[i])\n",
    "                all_masks.append(mask[i])\n",
    "        else:  # Do nothing\n",
    "            all_images.append(image)\n",
    "            all_masks.append(mask)\n",
    "    all_images = np.array(all_images)\n",
    "    all_masks = np.array(all_masks)\n",
    "    return [all_images, all_masks]\n",
    "\n",
    "\n",
    "# Pre-processing Data for training\n",
    "def preProcessAndSplitData(input_path, patch_x, patch_y, split_size=0.25, isSave=False, output=None):\n",
    "    all_images, all_masks = input_path\n",
    "    # Split data into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(all_images, all_masks, test_size=split_size, random_state=28)\n",
    "    if isSave:\n",
    "        if output == None:\n",
    "            print(\"ERROR: provide output. Folder where patched and split dataset will be saved\")\n",
    "        for i in range(0, len(x_train)):\n",
    "            im = np.array(x_train[i])\n",
    "            msk = np.array(y_train[i].reshape((patch_x, patch_y)))\n",
    "            plt.imsave(output + '/x_train/train/' + str(i) + '.jpg',im)\n",
    "            plt.imsave(output + '/y_train/train/' + str(i) + '.png',msk)\n",
    "        for i in range(0, len(x_val)):\n",
    "            im = np.array(x_val[i])\n",
    "            msk = np.array(y_val[i].reshape((patch_x, patch_y)))\n",
    "            plt.imsave(output + '/x_val/val/' + str(i) + '.jpg',im)\n",
    "            plt.imsave(output + '/y_val/val/' + str(i) + '.png',msk)\n",
    "    # preprocess input\n",
    "    x_train = preprocess_input(x_train)\n",
    "    x_val = preprocess_input(x_val)\n",
    "    return [x_train, x_val, y_train, y_val]\n",
    "\n",
    "\n",
    "# useAlpha: images come in 4 channels. Do you want to apply alpha channel to rgb?\n",
    "def openAndProcessHDRimage(path, useAlpha=False, isTesting=False, useLog=True, cutGround=None):\n",
    "    img = sio.read(path)\n",
    "    dims = len(img.shape)\n",
    "    if dims == 4:  # There is alpha\n",
    "        img = np.reshape(img, (img.height, img.width, 4))\n",
    "        if useAlpha:\n",
    "            img[:, :, 0] = img[:, :, 3] * img[:, :, 0]\n",
    "            img[:, :, 1] = img[:, :, 3] * img[:, :, 1]\n",
    "            img[:, :, 2] = img[:, :, 3] * img[:, :, 2]\n",
    "        img = np.delete(img, 3, 2)\n",
    "    x = 1\n",
    "    y = 1\n",
    "    if isTesting:\n",
    "        print(\"before log\")\n",
    "        print(img[x, y, :])\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    if useLog:\n",
    "        img_new = preProcessHDRdata(img)\n",
    "    else:\n",
    "        img_new = img\n",
    "\n",
    "    if isTesting:\n",
    "        print(\"after log\")\n",
    "        print(img_new[x, y, :])\n",
    "        print(img_new.shape)\n",
    "        plt.imshow(img_new)\n",
    "        plt.show()\n",
    "    # Cut bottom part of image\n",
    "    if cutGround:\n",
    "        img_new = img_new[:cutGround, :]\n",
    "    return img_new\n",
    "\n",
    "\n",
    "# Processing functions\n",
    "\n",
    "# Log transformation function for HDR input\n",
    "# Taken from SkyGAN project: https://diglib.eg.org/handle/10.2312/sr20221151\n",
    "def preProcessHDRdata(image):\n",
    "    transform_cfg = dnnlib.EasyDict(\n",
    "        # Multiplier value applied before the log transform\n",
    "        input_mul=1,  # e.g. 2**8 would shift it 8 EV steps up\n",
    "        # The epsilon constant for log-mapping input HDR images.\n",
    "        log_epsilon=1e-3,\n",
    "        # separate the value space around 1.0 into two separate mapping functions\n",
    "        # - x < 1: log(x) => expansion of the value space\n",
    "        # - x >= 1: pow(x, 1./log_pow) => adjustable compression of the value space\n",
    "        log_split_around1=False,\n",
    "        log_pow=7.5,\n",
    "        # Shift the value up/down (after the log transform) - can be used to ensure zero-mean\n",
    "        output_bias=2.5,\n",
    "        output_scale=1 / 2.2 / 2,\n",
    "    )\n",
    "    new_image = log_transform(image, transform_cfg)\n",
    "    return new_image\n",
    "\n",
    "\n",
    "# Applies the log-transformation that converts linear HDR images to a form suitable for a\n",
    "# neural network.\n",
    "def log_transform(x, transform_cfg):\n",
    "    if transform_cfg.input_mul != 1.0:\n",
    "        x = x * transform_cfg.input_mul\n",
    "    x = x + transform_cfg.log_epsilon\n",
    "\n",
    "    log_x = np.log(x)\n",
    "    pow_x = np.power(x, 1. / transform_cfg.log_pow) - 1.0\n",
    "\n",
    "    if transform_cfg.log_split_around1:\n",
    "        x = np.where(x < 1.0, log_x, pow_x)\n",
    "    else:\n",
    "        x = log_x\n",
    "    return (x + transform_cfg.output_bias) * transform_cfg.output_scale\n",
    "\n",
    "\n",
    "# Inverse of log_transform(x)\n",
    "def invert_log_transform(y, transform_cfg):\n",
    "    y = y / transform_cfg.output_scale - transform_cfg.output_bias\n",
    "\n",
    "    exp_y = np.exp(y)\n",
    "    pow_y = np.power(y + 1.0, transform_cfg.log_pow)\n",
    "\n",
    "    if transform_cfg.log_split_around1:\n",
    "        y = np.where(y < 0.0 / transform_cfg.input_mul, exp_y, pow_y)\n",
    "    else:\n",
    "        y = exp_y\n",
    "    y = y - transform_cfg.log_epsilon\n",
    "\n",
    "    if transform_cfg.input_mul != 1.0:\n",
    "        y = y / transform_cfg.input_mul\n",
    "    return y\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    if np.max(x) - np.min(x) == 0.0:\n",
    "        return x\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "\n",
    "def apply_log_clip(x):\n",
    "    return np.clip(preProcessHDRdata(x), 0, 1)\n",
    "\n",
    "\n",
    "def apply_log_tanh_normalize(x):\n",
    "    return normalize(np.tanh(preProcessHDRdata(x)))\n",
    "\n",
    "\n",
    "def apply_log_normalize(x):\n",
    "    return normalize(preProcessHDRdata(x))\n",
    "\n",
    "\n",
    "# parse COCO dataset, get HDR images and their masks, patch them, split into train and validation folders.\n",
    "# input - folder that contains COCO .json file and 'images' folder\n",
    "# output - folder where split dataset will be saved. Expected to already have training and validation folders of correct format.\n",
    "# Note: This function expects only 8K (8192x4096) input images and only in .exr format\n",
    "def createInitialHDRDataset(input_path, output, patch_x=224, patch_y=224, stride=100, isSplit=True, padding='SAME',\n",
    "                            cutGround=2048, isVerboseOn=False):\n",
    "    coco = COCO(input_path + 'result.json')  # use library to parse COCO .json file\n",
    "    img_dir = input_path + 'images/'  # folder with initial images\n",
    "    number_of_images = len(os.listdir(img_dir))\n",
    "    nonSkyCategory = 0  # define what value non-sky has\n",
    "    skyCategory = 1  # define what value sky has\n",
    "    isBinary = True\n",
    "    imgs = coco.imgs\n",
    "    cat_ids = coco.getCatIds()\n",
    "    count_exr = 0\n",
    "    count_patches = 1\n",
    "    # Count amount of images from Charles University dataset - they don't have lower part of picture\n",
    "    # Note: this is specific to the dataset used in the thesis. If you want to cut lower part in your images, then add UNI to the file names\n",
    "    # The images from Charles University dataset do not contain information on the lower part of them.\n",
    "    number_of_images_no_ground = 0\n",
    "    for img in imgs:\n",
    "        image_name = imgs[img]['file_name']\n",
    "        if (image_name.find(\"UNI\") != -1):  # The image is not from Charles University dataset\n",
    "            number_of_images_no_ground += 1\n",
    "    number_of_images_with_ground = number_of_images - number_of_images_no_ground\n",
    "\n",
    "    if (isSplit):\n",
    "        if (padding == 'SAME'):\n",
    "            # np.ceil is used to count also patches that will go overbound\n",
    "            # Uni images with cut ground\n",
    "            num_patches_no_ground_h = np.ceil((2048 - patch_x + stride) / stride)\n",
    "            num_patches_no_ground_w = np.ceil((8192 - patch_y + stride) / stride)\n",
    "            # Poly Haven images with full ground\n",
    "            num_patches_h = np.ceil((4096 - patch_x + stride) / stride)\n",
    "            num_patches_w = np.ceil((8192 - patch_y + stride) / stride)\n",
    "        else:\n",
    "            # np.floor is used to count only patches that will fit into original dimensions\n",
    "            # Uni images with cut ground\n",
    "            num_patches_no_ground_h = np.floor((2048 - patch_x + stride) / stride)\n",
    "            num_patches_no_ground_w = np.floor((8192 - patch_y + stride) / stride)\n",
    "            # Poly Haven images with full ground\n",
    "            num_patches_h = np.floor((4096 - patch_x + stride) / stride)\n",
    "            num_patches_w = np.floor((8192 - patch_y + stride) / stride)\n",
    "        number_of_patches_with_ground = number_of_images_with_ground * num_patches_h * num_patches_w\n",
    "        number_of_patches_no_ground = number_of_images_no_ground * num_patches_no_ground_h * num_patches_no_ground_w\n",
    "        total_array = np.arange(1, number_of_patches_with_ground + number_of_patches_no_ground, 1)\n",
    "        images_indexes = np.copy(total_array)\n",
    "        masks_indexes = np.copy(total_array)\n",
    "        # split dataset into train and val\n",
    "        # This is memory efficient solution. patches will be cut and numbers will be assigned to them in increasing order - indexes.\n",
    "        # Here we split only these indexes. Later patch with defined index will be assigned to corresponding set\n",
    "        x_train, x_val, y_train, y_val = train_test_split(images_indexes, masks_indexes, test_size=0.25,\n",
    "                                                          random_state=28)\n",
    "        # We do not need y indexes, they are the same as from x.\n",
    "        del y_train, y_val, images_indexes, masks_indexes, total_array, num_patches_no_ground_h, num_patches_no_ground_w, num_patches_h, num_patches_w, number_of_patches_with_ground, number_of_patches_no_ground\n",
    "\n",
    "    # specific image img'\n",
    "    for img in imgs:\n",
    "        image_name = imgs[img]['file_name']\n",
    "        # Check if image is from university's dataset - has no ground\n",
    "        if \"UNI\" in image_name:\n",
    "            cutGround = 2048\n",
    "        else:\n",
    "            cutGround = None\n",
    "        # Don't use preprocessing now. We will use it in Generator when opening images. Every patch will be processed by itself.\n",
    "        # That is why useLog = False\n",
    "        image = openAndProcessHDRimage(img_dir + image_name, useAlpha=False, useLog=False,\n",
    "                                       cutGround=cutGround)\n",
    "        image_name = image_name.replace('.exr', '')\n",
    "        if verbose:\n",
    "            print(image_name + ' in progress')\n",
    "        anns_ids = coco.getAnnIds(imgIds=imgs[img]['id'], catIds=cat_ids, iscrowd=None)\n",
    "        anns = coco.loadAnns(anns_ids)\n",
    "        mask = coco.annToMask(anns[0])\n",
    "        for i in range(len(anns)):\n",
    "            maskedAnns = coco.annToMask(anns[i])\n",
    "            if (anns[i]['category_id'] == nonSkyCategory):\n",
    "                # In manual annotation, label with value 2 was used to represent non-sky. Here we set it back to 0.\n",
    "                mask[np.logical_and(maskedAnns > 0, mask < 2)] = nonSkyCategory  # Set to non-sky\n",
    "            else:\n",
    "                mask[np.logical_and(maskedAnns > 0, mask < 2)] = skyCategory  # Set to sky\n",
    "        if (isBinary):\n",
    "            mask = np.minimum(mask, 1)\n",
    "        # Patch image and its mask\n",
    "        image = np.array(image)\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        patches_images = tf.image.extract_patches(images=image,\n",
    "                                                  sizes=[1, patch_x, patch_y, 1],\n",
    "                                                  strides=[1, stride, stride, 1],\n",
    "                                                  rates=[1, 1, 1, 1],\n",
    "                                                  padding=padding)\n",
    "        del image\n",
    "        patches_masks = tf.image.extract_patches(images=mask,\n",
    "                                                 sizes=[1, patch_x, patch_y, 1],\n",
    "                                                 strides=[1, stride, stride, 1],\n",
    "                                                 rates=[1, 1, 1, 1],\n",
    "                                                 padding=padding)\n",
    "        del mask\n",
    "        # Save patches\n",
    "        for i in range(patches_images.shape[1]):\n",
    "            for j in range(patches_images.shape[2]):\n",
    "                if (isSplit):\n",
    "                    if (count_patches in x_train):\n",
    "                        # this patch should be in training set.\n",
    "                        path_image_patch = os.path.join(output , 'x_train/train/')\n",
    "                        path_mask_patch = os.path.join(output ,'y_train/train/')\n",
    "                    else:\n",
    "                        # this patch should be in validation set.\n",
    "                        path_image_patch = os.path.join(output,'x_val/val/')\n",
    "                        path_mask_patch = os.path.join(output ,'y_val/val/')\n",
    "                # Save image patch\n",
    "                single_patch_image = tf.reshape(patches_images[0, i, j], (patch_x, patch_y, 3))\n",
    "                single_patch_image = np.array(single_patch_image)\n",
    "                # Save as .tif file\n",
    "                tifImsave(os.path.join(path_image_patch, str(count_patches) + '.tif'), single_patch_image)\n",
    "                # Save mask patch\n",
    "                single_patch_mask = tf.reshape(tf.reshape(patches_masks[0, i, j], (patch_x, patch_y, 1)),\n",
    "                                               (patch_x, patch_y))\n",
    "                single_patch_mask = np.array(single_patch_mask)\n",
    "                msk = Image.fromarray(single_patch_mask)\n",
    "                # Save as .png file\n",
    "                msk.save(os.path.join(path_mask_patch, str(count_patches) + '.png'))\n",
    "                # Preserve same name between patches of masks and images\n",
    "                count_patches = count_patches + 1\n",
    "                del single_patch_image, single_patch_mask, msk\n",
    "        del patches_images, patches_masks\n",
    "        count_exr = count_exr + 1\n",
    "    if isVerboseOn:\n",
    "        print(str(count_patches) + ' patches were created from ' + str(count_exr) + ' images')\n",
    "\n",
    "\n",
    "### Working with Model\n",
    "def getModel(BACKBONE, showModel=False):\n",
    "    # define model\n",
    "    model = sm.Unet(BACKBONE, encoder_weights='imagenet')\n",
    "    focal_loss = sm.losses.BinaryFocalLoss(gamma=2.0)\n",
    "    model.compile('Adam', loss=focal_loss, metrics=[sm.metrics.iou_score])\n",
    "    if (showModel):\n",
    "        print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def getOldModel(model, showModel=False):\n",
    "    # Define old metrics that were used in training\n",
    "    # Note you can add your own metrics if other were used\n",
    "    old_loss = sm.losses.bce_jaccard_loss\n",
    "    focal_loss = sm.losses.BinaryFocalLoss(gamma=2.0)\n",
    "    model = keras.models.load_model(model,\n",
    "                                    custom_objects={'Functional': keras.models.Model,\n",
    "                                                    'binary_crossentropy_plus_jaccard_loss': old_loss,\n",
    "                                                    'binary_focal_loss': focal_loss, 'iou_score': sm.metrics.iou_score},\n",
    "                                    compile=True)\n",
    "    model.compile('Adam', loss=focal_loss, metrics=[sm.metrics.iou_score])\n",
    "    if (showModel):\n",
    "        print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# This function trains the model with generator\n",
    "# data - array containing generators\n",
    "def trainModelGenerator(model, data, epochs, steps_per_epoch=None, callbacks=[]):\n",
    "    train_generator, val_generator = data\n",
    "    if (steps_per_epoch == None):\n",
    "        print(\"ERROR: define steps_per_epoch!\")\n",
    "        return\n",
    "    history = model.fit(train_generator, validation_data=val_generator,\n",
    "                        steps_per_epoch=steps_per_epoch, epochs=epochs, callbacks=callbacks)\n",
    "    return [model, history]\n",
    "\n",
    "\n",
    "# This function trains the model without generator\n",
    "# data - array containing loaded split dataset\n",
    "def trainModelWithoutGenerator(model, data, epochs, batch_size=None, steps_per_epoch=None, verbose=1, callbacks=[]):\n",
    "    x_train, x_val, y_train, y_val = data\n",
    "    # fit model\n",
    "    if (batch_size):\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    else:\n",
    "        history = model.fit_generator(train_generator, validation_data=val_generator,\n",
    "                                      steps_per_epoch=steps_per_epoch,\n",
    "                                      validation_steps=steps_per_epoch, epochs=epochs)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=steps_per_epoch,\n",
    "            verbose=verbose,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    return [model, history]\n",
    "\n",
    "\n",
    "# This function was used to predict LDR input\n",
    "# output - folder in which result will be saved\n",
    "# input - path to the image\n",
    "def predictLDR(input_path, output, patch_x, patch_y, stride, model=None, verbose=True):\n",
    "    image_name = input_path.split(os.sep)[-1]  # For saving segmentation result\n",
    "    image = np.array(Image.open(input_path))\n",
    "    if verbose:\n",
    "        printAnalyzeImage(image)\n",
    "    # Save original image dimensions\n",
    "    img_x = image.shape[0]\n",
    "    img_y = image.shape[1]\n",
    "    # Prepare image for prediction - break into 512 patches\n",
    "    patched_image = patchImage(image, (patch_x, patch_y), stride, isMask=False, isNormalize=True)\n",
    "    amountOfPatches = len(patched_image)\n",
    "    prediction = model.predict(patched_image, verbose=1)\n",
    "    del patched_image\n",
    "    predicted = prediction.reshape((amountOfPatches, patch_x, patch_y))\n",
    "    predicted = np.where(predicted > 0.5, 1, 0).astype('uint8')\n",
    "    result = reconstructMask(predicted, [img_x, img_y], [patch_x, patch_y], stride)\n",
    "    del predicted\n",
    "    fileFormat = image_name.split('.')[-1]\n",
    "    image_name = image_name.replace(\".\" + fileFormat, \"\")\n",
    "    image_name = image_name + \"_segmented.png\"\n",
    "    if verbose:\n",
    "        print(\"=== Saving model ===\")\n",
    "        print(\"in \" + os.path.join(output, image_name))\n",
    "    plt.imsave(os.path.join(output, image_name), result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# This function merges patches back to full-sized image and considers overlapping.\n",
    "# It improves prediction results, as overlapping regions are predicted several times.\n",
    "def reconstructMask(patches, original_dim, patch_dim, s, method='voting'):\n",
    "    i_h, i_w = original_dim\n",
    "    p_h, p_w = patch_dim\n",
    "    # amount of patches\n",
    "    amount_h = np.floor((i_h - p_h + s) / s).astype(int)\n",
    "    amount_w = np.floor((i_w - p_w + s) / s).astype(int)\n",
    "    count = 0\n",
    "    # Voting requires prediction to be of values 1 and 0 (uint8 type)\n",
    "    result = np.zeros((i_h, i_w), dtype='uint8' if method == 'voting' else 'float32')\n",
    "    voting = np.zeros((i_h, i_w), dtype='int')\n",
    "    for h in range(0, amount_h):\n",
    "        for w in range(0, amount_w):\n",
    "            ind_h = h * s\n",
    "            ind_w = w * s\n",
    "            # get part of image where patch should be\n",
    "            part_of_image = result[ind_h:ind_h + p_h, ind_w: ind_w + p_w]\n",
    "            # get patch that corresponds to this part\n",
    "            patch = np.array(patches[count])\n",
    "            count += 1\n",
    "            # Voting method is the only recommended method. Others do not show good results\n",
    "            if method == 'voting':\n",
    "                # get same patch place in voting array\n",
    "                vote_result = voting[ind_h:ind_h + p_h, ind_w: ind_w + p_w]\n",
    "                # where prediction was 1, increase vote for this pixel to be a sky. Otherwise, decrease\n",
    "                vote_result = np.where(patch == 1, vote_result + 1, vote_result - 1)\n",
    "                # Save updated patch place in voting array\n",
    "                voting[ind_h:ind_h + p_h, ind_w: ind_w + p_w] = vote_result\n",
    "            else:  # Not recommended\n",
    "                # get part of image where patch should be\n",
    "                if method == 'max':\n",
    "                    part_of_image = np.maximum(part_of_image, patch)\n",
    "                elif method == 'min':\n",
    "                    part_of_image = np.minimum(part_of_image, patch)\n",
    "                elif method == 'avg':\n",
    "                    # we gradually add values of patches to original-sized image. If part of image doesn't contain patch\n",
    "                    # info yet, we don't calculate average for it, but only get value of the patch\n",
    "                    part_of_image = np.where(part_of_image == 0, patch, part_of_image)\n",
    "                    if result.dtype == 'uint8':\n",
    "                        part_of_image = (np.add(part_of_image, patch)) // 2\n",
    "                    else:\n",
    "                        part_of_image = (np.add(part_of_image, patch)) / 2\n",
    "            result[ind_h:ind_h + p_h, ind_w: ind_w + p_w] = part_of_image\n",
    "    if method == 'voting':\n",
    "        # Now make final image. If pixel was voted to be sky, it will be set as sky.\n",
    "        result = np.where(voting > 0, 1, 0)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Get patches, predict, apply voting, return prediction\n",
    "def predictSplitHDRImage(path_patches, path_result, model, img_x, img_y, save=True, save_name='segmented',\n",
    "                         patch_x=224, patch_y=224,\n",
    "                         stride=100, is_normalize=True, verbose=True):\n",
    "    # Images are patched in ascending order\n",
    "    test_list = [x.split('.')[0] for x in os.listdir(path_patches)]\n",
    "    test_list = [int(x) for x in test_list]\n",
    "    test_list = sorted(test_list)\n",
    "    amount_of_patches = len(test_list)\n",
    "    print(amount_of_patches)\n",
    "    imgs = []\n",
    "    for file in test_list:\n",
    "        img_tif = tifImread(os.path.join(path_patches,str(file) + '.tif'))\n",
    "        if is_normalize:\n",
    "            # Process the input.\n",
    "            # Note you can change this to experiment with other processing\n",
    "            img_tif = apply_log_normalize(img_tif)\n",
    "        imgs.append(img_tif)\n",
    "        del img_tif\n",
    "\n",
    "    imgs = np.array(imgs)\n",
    "    if verbose:\n",
    "        print(\"===Getting Data Finished===\")\n",
    "        print(\"===Prediction Started===\")\n",
    "    prediction = model.predict(imgs, verbose=verbose)\n",
    "    del imgs\n",
    "    predicted = prediction.reshape((amount_of_patches, patch_x, patch_x))\n",
    "    predicted = np.where(predicted > 0.5, 1, 0).astype('uint8')\n",
    "    result = reconstructMask(predicted, [img_x, img_y], [patch_x, patch_y], stride)\n",
    "    if verbose:\n",
    "        print(\"===Prediction Finished===\")\n",
    "    if save:\n",
    "        if verbose:\n",
    "            print(\"=== Saving result ===\")\n",
    "            print(\"in: \" + os.path.join(path_result, save_name + '.png'))\n",
    "        plt.imsave(os.path.join(path_result, save_name + '.png'), result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def HdrSplit(path_exr, path_patches, patch_x=224,\n",
    "             patch_y=224, stride=100, useLog=False, verbose=True):\n",
    "    \n",
    "    # useLog is False by default. We apply preprocessing at individual patches, not once on full picture\n",
    "    image = openAndProcessHDRimage(path_exr, useAlpha=False, isTesting=False, useLog=useLog)\n",
    "\n",
    "    if verbose:\n",
    "        printAnalyzeImage(image)\n",
    "\n",
    "    # Save original image dimensions for future use\n",
    "    [img_x, img_y, _] = image.shape\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    # Get filename of .exr\n",
    "    # We first get filename with extension, then split it by '.' dot and get only name, ignoring extension\n",
    "    filename = os.path.split(path_exr)[1].split('.')[0]\n",
    "\n",
    "    # Create folder with patches if it doesn't exist yet\n",
    "    path_patches = os.path.join(path_patches, filename + '_patches')\n",
    "    if not os.path.isdir(path_patches):\n",
    "        try:\n",
    "            os.makedirs(path_patches)\n",
    "        except OSError as error:\n",
    "            print(error)\n",
    "            return\n",
    "    elif len(os.listdir(path_patches)) > 0:\n",
    "        # Patches were already created\n",
    "        return [path_patches, img_x, img_y]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"==== START PREPROCESS ====\")\n",
    "\n",
    "    # Patch image\n",
    "    patches_images = tf.image.extract_patches(images=image,\n",
    "                                              sizes=[1, patch_x, patch_y, 1],\n",
    "                                              strides=[1, stride, stride, 1],\n",
    "                                              rates=[1, 1, 1, 1],\n",
    "                                              padding='VALID')\n",
    "    del image\n",
    "    count_patches = 1\n",
    "    for i in range(patches_images.shape[1]):\n",
    "        for j in range(patches_images.shape[2]):\n",
    "            # Save image patch\n",
    "            single_patch_image = tf.reshape(patches_images[0, i, j], (patch_x, patch_y, 3))\n",
    "            single_patch_image = np.array(single_patch_image)\n",
    "            tifImsave(os.path.join(path_patches, str(count_patches) + '.tif'), single_patch_image)\n",
    "            count_patches = count_patches + 1\n",
    "            del single_patch_image\n",
    "    del patches_images\n",
    "    return [path_patches, img_x, img_y]\n",
    "\n",
    "\n",
    "def predictHdr(input_path, output, patch_x, patch_y, stride, model=None, verbose=True):\n",
    "    # Determine what is input. File or folder with splitted patches\n",
    "    if '.exr' in input_path and os.path.isfile(input_path):\n",
    "        # It is .exr file. We need to patch it and predict\n",
    "        input_path, img_x, img_y = HdrSplit(input_path, \n",
    "                                            output, \n",
    "                                            patch_x=patch_x, \n",
    "                                            patch_y=patch_y, \n",
    "                                            stride=stride,\n",
    "                                            useLog=False,\n",
    "                                            verbose=verbose)\n",
    "\n",
    "    # Load model if it was defined as a path\n",
    "    if isinstance(model, str):\n",
    "        # Use model user provided. Must be .h5 file\n",
    "        model = keras.models.load_model(model,\n",
    "                                        custom_objects={'Functional': keras.models.Model}, compile=False)\n",
    "    if model==None:\n",
    "        print(\"ERROR: Model must be defined!\")\n",
    "        return\n",
    "    # Input should be folder with patches. If it is not, something is wrong\n",
    "    if os.path.isdir(input_path) and '.tif' in os.listdir(input_path)[0]:\n",
    "        # It is folder with .tif files\n",
    "        result = predictSplitHDRImage(input_path, output, model, img_x=img_x, img_y=img_y, save=True,\n",
    "                                      save_name='segmented',\n",
    "                                      patch_x=patch_x,\n",
    "                                      patch_y=patch_y,\n",
    "                                      stride=stride, verbose=verbose)\n",
    "    else:\n",
    "        print(\"ERROR: Folder with patches was not found. Check provided path\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# This function tests generator for sanity\n",
    "def testGenerator(generator):\n",
    "    print(len(generator))  # Amount of batches in generator\n",
    "    count = 0\n",
    "    for i in range(0, len(generator)):\n",
    "        if np.array_equiv(np.unique(generator[i][1][0]), [0., 1.]):  # Find only pairs with both sky and ground (non-sky)\n",
    "            count += 1\n",
    "            element = generator[i]\n",
    "            plt.imshow(element[1][0])\n",
    "            plt.show()\n",
    "            plt.imshow(element[0][0])\n",
    "            plt.show()\n",
    "            print(np.unique(element[1][0]))\n",
    "            if count == 3:  # Show only 3 pairs of image and mask that contain both sky and ground (non-sky)\n",
    "                break\n",
    "\n",
    "# General tools\n",
    "def printMinMax(x):\n",
    "    print(\"min max: %.4f %.4f\" % (np.min(x), np.max(x)))\n",
    "\n",
    "\n",
    "def showImage(x):\n",
    "    plt.imshow(x)\n",
    "    plt.show()\n",
    "\n",
    "# Print some information about image and show it\n",
    "def printAnalyzeImage(x):\n",
    "    showImage(x)\n",
    "    printMinMax(x)\n",
    "    print()\n",
    "\n",
    "# This function quickly returns desired resolutions\n",
    "def getSizes(size=\"1080p\"):\n",
    "    if (size == '8K'):\n",
    "        return [4096, 8192]\n",
    "    if (size == '4K'):\n",
    "        return [2160, 3840]\n",
    "    if (size == '2K'):\n",
    "        return [1080, 2048]\n",
    "    if (size == '1080p'):\n",
    "        return [1080, 1920]\n",
    "    if (size == '720p'):\n",
    "        return [720, 1280]\n",
    "    if (size == '1080p'):\n",
    "        return [480, 852]\n",
    "    if (size == '720sq'):\n",
    "        return [720, 720]\n",
    "    if (size == '512sq'):\n",
    "        return [512, 512]\n",
    "    if (size == '256sq'):\n",
    "        return [256, 256]\n",
    "    if (size == '224sq'):\n",
    "        return [224, 224]\n",
    "\n",
    "# Helper function for the \"countTypes\"\n",
    "def countTypes_subFunction(path, files):\n",
    "    sky = [1]  # Only sky\n",
    "    skyGround = np.array([0, 1])  # Both sky and ground (non-sky)\n",
    "    skyCount = 0\n",
    "    groundCount = 0\n",
    "    skyGroundCount = 0\n",
    "    for file in files:\n",
    "        mask = np.array(Image.open(path + file))  # Open mask\n",
    "        mask = np.unique(mask)  # Get only unique values in the array\n",
    "        if np.array_equal(mask, skyGround):  # This mask contains both sky and ground\n",
    "            skyGroundCount += 1\n",
    "        elif np.array_equal(mask, sky):  # This mask contains only sky\n",
    "            skyCount += 1\n",
    "        else:  # This mask contains only ground\n",
    "            groundCount += 1\n",
    "    print('Only Sky: ', str(skyCount))\n",
    "    print('Only Ground: ', str(groundCount))\n",
    "    print('Both Sky and Ground (Horizon): ', str(skyGroundCount))\n",
    "\n",
    "\n",
    "# This function counts amount of patches in the dataset that contain only sky, only ground or both sky and ground\n",
    "# path - path to the dataset\n",
    "def countTypes(path):\n",
    "    # Masks already represent true values, so it is easy to count.\n",
    "    # Takes some time, when large dataset\n",
    "    train = path + 'y_train/train/'\n",
    "    val = path + 'y_val/val/'\n",
    "    files_train = os.listdir(train)\n",
    "    print(\"==== Training set ====\")\n",
    "    print(\"Total: \", str(len(files_train)))\n",
    "    countTypes_subFunction(train, files_train)\n",
    "\n",
    "    files_val = os.listdir(val)\n",
    "    print(\"==== Validation set ====\")\n",
    "    print(\"Total: \", str(len(files_val)))\n",
    "    countTypes_subFunction(val, files_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "522e1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up augmentations for Albumentations library\n",
    "def defineAugmentation():\n",
    "    augmentation = [\n",
    "        A.HorizontalFlip(p=0.5),  # Mirros the image with a probability of 50%\n",
    "\n",
    "        # Shift, scale and rotate image in 40% of images\n",
    "        A.ShiftScaleRotate(scale_limit=0.2, shift_limit=0.2, rotate_limit=25, border_mode=cv2.BORDER_REFLECT_101,\n",
    "                           p=0.4),\n",
    "        A.Perspective(scale=0.1, p=0.2, pad_mode=cv2.BORDER_REFLECT_101),\n",
    "        # Change perspective in 20% of images. Note: Creates random mistakes when big scale\n",
    "        # Apply blur in 40% of images\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Blur(p=1),\n",
    "                A.MotionBlur(p=1),\n",
    "            ],\n",
    "            p=0.4,\n",
    "        ),\n",
    "    ]\n",
    "    return A.Compose(augmentation)\n",
    "\n",
    "\n",
    "# Create Custom Data Generator for HDR images.\n",
    "# Reading .tif files from directory\n",
    "# Expects both X (patches) and y (masks) folders\n",
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "\n",
    "    # This function is called once the generator is initiated\n",
    "    def __init__(self, X_path, y_path,\n",
    "                 batch_size=16,\n",
    "                 input_size=(224, 224),\n",
    "                 input_channels=3,\n",
    "                 shuffle=True,\n",
    "                 processing='log_norm'):\n",
    "\n",
    "        self.X_path = X_path\n",
    "        self.y_path = y_path\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.input_channels = input_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.processing = processing\n",
    "        # get list of file names without extensions. names for X and y should be the same. They are usually just numbers from 1.\n",
    "        # X must have .tif extension and y .png\n",
    "        self.list_files = [x.split('.')[0] for x in os.listdir(X_path)]\n",
    "        self.total_length = len(self.list_files)\n",
    "        self.augmentation = defineAugmentation()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    # Called once at the end of epoch\n",
    "    def on_epoch_end(self):\n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = np.arange(self.total_length)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)  # Read input in different order\n",
    "\n",
    "    # The function reads images and constructs X and y sets with size of batch_size\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.input_size, self.input_channels), dtype='float32')\n",
    "        y = np.empty((self.batch_size, *self.input_size, 1),\n",
    "                     dtype='float32')  # masks are fed into network as float32 type, yet they have only values 0.0 and 1.0\n",
    "        # Fill in a batch\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Open the .tif file with a patch\n",
    "            image = tifImread(self.X_path + ID + '.tif')\n",
    "            # Open the file with a mask. Initially it creates 2D array without any channel, so we expand the dimension.\n",
    "            mask = np.expand_dims(np.array(Image.open(self.y_path + ID + '.png'), dtype='float32'), axis=-1)\n",
    "            # Apply augmentation\n",
    "            # The Albumentation library ensures the mask's values stay to be only 0.0 and 1.0\n",
    "            output = self.augmentation(image=image, mask=mask)\n",
    "            # It is important to first apply augmentations and then clip to range between 0 and 1\n",
    "            # Otherwise, we would get values outside of 0...1 range, as augmentations don't consider it\n",
    "            if self.processing == 'log_norm':\n",
    "                X[i,] = apply_log_normalize(output['image'])  # Apply log transformation, then normalize to 0...1\n",
    "            elif self.processing == 'log_tanh_norm':\n",
    "                X[i,] = apply_log_tanh_normalize(\n",
    "                    output['image'])  # Apply log transformation, then tanh, then normalize to 0...1\n",
    "            else:\n",
    "                X[i,] = output['image']  # Do not process input\n",
    "            y[i,] = output['mask']  # Masks are not processed\n",
    "        return X, y\n",
    "\n",
    "    # Called when new batch of images is needed in training.\n",
    "    def __getitem__(self, index):\n",
    "        # X - NumPy array of patches [batch_size, patch_height, patch_width, patch_channel]\n",
    "        # y - NumPy array of masks [batch_size, patch_height, patch_width]\n",
    "\n",
    "        # Generate indexes of this new batch. We already defined order of images in on_epoch_end function and stored to self.indexes\n",
    "        # Here we just get correct amount of images\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of file names (Each file name should be a number, or index)\n",
    "        list_files_temp = [self.list_files[k] for k in indexes]\n",
    "\n",
    "        # Construct X and y\n",
    "        X, y = self.__data_generation(list_files_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    # Defines amount of batches\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.total_length / self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2b5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paths\n",
    "# DATA_DIR will be concatenaded with other folders provided below\n",
    "DATA_DIR = 'Enter path to the folder where your datasets are located/'\n",
    "# This variable is used if you plan to use already split into train/val dataset. If you only plan to create the\n",
    "# dataset and split it, then this variable should point to the folder with coco .json file and folder 'images'.\n",
    "# Switch the boolean 'isCreatingDatasetOn' below if that is the case.\n",
    "# The folder 'patches' in the TRAIN_FOLDER will be created where split dataset will be saved.\n",
    "TRAIN_FOLDER = 'Enter name of the folder with the dataset/'\n",
    "# Note MODEL_FOLDER should contain 'logs' folder inside if you want to save logs\n",
    "MODEL_FOLDER = 'Enter path to the folder with saved models/'\n",
    "PREDICTION_PATH = 'Enter path with the image you want to predict'\n",
    "PREDICTION_SAVE_FOLDER = 'Enter folder where you want to save prediciton/'\n",
    "PREDICTION_NAME = 'Enter name of the prediction/' to save prediciton/'\n",
    "PREDICTION_NAME = 'Enter name of the prediction/'\n",
    "\n",
    "# Resizing initial images is not recommended, it is better to patch the image\n",
    "resize = False\n",
    "# Cut inital image into small patches?\n",
    "patch = True\n",
    "# Size of patches (or of desired resizing)\n",
    "patch_x, patch_y = getSizes(\"224sq\")\n",
    "# Amount of pixels for a Step (stride) between patches. Smaller - the better.\n",
    "# However, it takes more memory to create more patches.\n",
    "stride = 100\n",
    "\n",
    "### Model info\n",
    "# Amount of epochs to train the model\n",
    "epochs = 1\n",
    "# Batch size - how many images will be loaded into network simultaniously. Bigger batch sizes require more memory\n",
    "batch_size = 16\n",
    "# Should network print logs?\n",
    "verbose = 1\n",
    "# is task - binary classification?\n",
    "isBinary = True\n",
    "# Backbone of the model to use for training\n",
    "# For more options check here: https://github.com/qubvel/segmentation_models/blob/master/segmentation_models/backbones/backbones_factory.py\n",
    "BACKBONE = 'resnet34'\n",
    "# Get preprocess function of backbone. NOTE: preprocess of resnet34 does not do anything.\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "# Otherwise, if you want to use another trained model as backbone, enter its name with .h5 extension. It will be taken from the MODEL_FOLDER\n",
    "# Leave this variable empty to use BACKBONE instead\n",
    "OLD_MODEL_NAME = None\n",
    "# Name of the file the final trained model will be saved as (without .h5 extension)\n",
    "# Can be editted\n",
    "MODEL_NAME = 'model_{}_{}ep_{}bch_{}X_{}Y_2'.format(BACKBONE, epochs, batch_size,\n",
    "                                                    patch_x, patch_y)\n",
    "### Callbacks\n",
    "\n",
    "# Initialize tensorboard that will be saving logs of the training process. It saves loss values and IoU score.\n",
    "# It creates the model logs folder in 'logs' by itself\n",
    "tensorBoard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=MODEL_FOLDER + 'logs/' + BACKBONE + 'testingcode_ep' + str(epochs) + '_' + str(patch_x) + 'x' + str(\n",
    "        patch_y) + '_batch' + str(batch_size) + '/fit/',\n",
    "    histogram_freq=1, write_graph=True, update_freq=\"epoch\")\n",
    "\n",
    "# Initialize checkpoints. This callback will be saving model every time validation IoU score will be the highest so far.\n",
    "# Requires disk space, every saved model of resnet34 weighs more than 200MB.\n",
    "\n",
    "# With what name to save the checkpoints of the model?\n",
    "MODEL_CHECKPOINT_NAME = MODEL_NAME + '.' + BACKBONE + '.b' + str(\n",
    "    batch_size) + '.epoch{epoch:02d}-focalLoss{val_loss:.6f}-iou{val_iou_score:.6f}.h5'\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_FOLDER + MODEL_CHECKPOINT_NAME,\n",
    "    verbose=1,  # Prints when checkpoint is saved\n",
    "    monitor=\"val_iou_score\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"max\")  # Save model if val_iou_score is highest so far\n",
    "callbacks = [cp_callback, tensorBoard]\n",
    "\n",
    "### Switches\n",
    "isTrainingOn = True  # Train model?\n",
    "isVerboseOn = True  # Output extra information while the code is executed? Good for testing\n",
    "isPredictionOn = True  # Use model to make a prediction?\n",
    "isCreatingDatasetOn = True  # Create new dataset before training?\n",
    "useStandardDataset = False  # Load full dataset into memory, don't use generators. Not recommended. HDR is not supported!\n",
    "useGeneratorDataset = True  # Use generator dataset. Supports augmentation of images\n",
    "isHdr = True  # input is in HDR format (.exr)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "707360bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "UNI_farm_vystice11 in progress\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1,21,82,150528] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ExtractImagePatches]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m path_in \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR, TRAIN_FOLDER)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isHdr:\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mcreateInitialHDRDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                            \u001b[49m\u001b[43misSplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutGround\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misVerboseOn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43misVerboseOn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Update TRAIN_FOLDER to point into split dataset\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     TRAIN_FOLDER \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR \u001b[38;5;241m+\u001b[39m TRAIN_FOLDER, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatches\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mcreateInitialHDRDataset\u001b[1;34m(input_path, output, patch_x, patch_y, stride, isSplit, padding, cutGround, isVerboseOn)\u001b[0m\n\u001b[0;32m    350\u001b[0m mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    351\u001b[0m image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(image, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 352\u001b[0m patches_images \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43msizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m image\n\u001b[0;32m    358\u001b[0m patches_masks \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mextract_patches(images\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    359\u001b[0m                                          sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, patch_x, patch_y, \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    360\u001b[0m                                          strides\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, stride, stride, \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    361\u001b[0m                                          rates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    362\u001b[0m                                          padding\u001b[38;5;241m=\u001b[39mpadding)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\segModels39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\segModels39\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7163\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,21,82,150528] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ExtractImagePatches]"
     ]
    }
   ],
   "source": [
    "# Create new dataset\n",
    "if isCreatingDatasetOn:\n",
    "    # Create folder \"patches\"\n",
    "    # Inside \"patches\" create folders for training and validation sets in expected format\n",
    "    # Create correct split folders of training and validation sets in output path\n",
    "    try:\n",
    "        output = os.path.join(DATA_DIR, TRAIN_FOLDER)\n",
    "        if not os.path.isdir(output):\n",
    "            # Create folder with patches if it doesn't exist yet\n",
    "            os.makedirs(os.path.join(output, 'patches'))\n",
    "            output = os.path.join(output, 'patches')\n",
    "            path_patches = os.path.join(output, 'x_train')\n",
    "            os.makedirs(path_patches)\n",
    "            path_patches = os.path.join(os.path.join(output, 'x_train'), 'train')\n",
    "            os.makedirs(path_patches)\n",
    "            path_patches = os.path.join(output, 'y_train')\n",
    "            os.makedirs(path_patches)\n",
    "            path_patches = os.path.join(os.path.join(output, 'y_train'), 'train')\n",
    "            os.makedirs(path_patches)\n",
    "            path_patches = os.path.join(output, 'x_val')\n",
    "            os.makedirs(path_patches)\n",
    "            path_patches = os.path.join(os.path.join(output, 'x_val'), 'val')\n",
    "            os.makedirs(path_patches)\n",
    "            path_patches = os.path.join(output, 'y_val')\n",
    "            os.makedirs(path_patches)\n",
    "            path_patches = os.path.join(os.path.join(output, 'y_val'), 'val')\n",
    "            os.makedirs(path_patches)\n",
    "    except OSError as error:\n",
    "        raise SystemExit(error)\n",
    "    path_out = os.path.join(os.path.join(DATA_DIR, TRAIN_FOLDER), 'patches')\n",
    "    path_in = os.path.join(DATA_DIR, TRAIN_FOLDER)\n",
    "    if isHdr:\n",
    "        createInitialHDRDataset(path_in, path_out, stride=stride, patch_x=patch_x, patch_y=patch_y,\n",
    "                                isSplit=True, padding='SAME', cutGround=2048, isVerboseOn=isVerboseOn)\n",
    "        # Update TRAIN_FOLDER to point into split dataset\n",
    "        TRAIN_FOLDER = os.path.join(os.path.join(DATA_DIR,TRAIN_FOLDER), 'patches')\n",
    "    else:\n",
    "        # Create LDR dataset\n",
    "        all_images, all_masks = parseDatasetLDR(path_in, resize, patch_x, patch_y, isBinary, patch)\n",
    "        x_train, x_val, y_train, y_val = preProcessAndSplitData([all_images, all_masks], patch_x, patch_y,\n",
    "                                                                split_size=0.25, isSave=True, output=path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training set\n",
    "if isTrainingOn:\n",
    "    if useStandardDataset:\n",
    "        # HDR is not supported, since HDR datasets are too large to fit fully into memory\n",
    "        all_images, all_masks = parseDatasetLDR(DATA_DIR + TRAIN_FOLDER, resize, patch_x, patch_y, isBinary, patch)\n",
    "        x_train, x_val, y_train, y_val = preProcessAndSplitData([all_images, all_masks], patch_x, patch_y, split_size=0.25,\n",
    "                                                                isSave=False)\n",
    "        # Sanity check\n",
    "        showImage([x_train, y_train], 3)\n",
    "\n",
    "    if useGeneratorDataset:\n",
    "        if isHdr:\n",
    "            # Define custom HDR generator\n",
    "            params_train = {'X_path': os.path.join(DATA_DIR, TRAIN_FOLDER , 'x_train/train/'),\n",
    "                            'y_path': os.path.join(DATA_DIR , TRAIN_FOLDER , 'y_train/train/'),\n",
    "                            'batch_size': batch_size,\n",
    "                            'input_size': (patch_x, patch_y),\n",
    "                            'input_channels': 3,\n",
    "                            'shuffle': True}\n",
    "            params_val = {'X_path': os.path.join(DATA_DIR , TRAIN_FOLDER , 'x_val/val/'),\n",
    "                          'y_path': os.path.join(DATA_DIR , TRAIN_FOLDER , 'y_val/val/'),\n",
    "                          'batch_size': batch_size,\n",
    "                          'input_size': (patch_x, patch_y),\n",
    "                          'input_channels': 3,\n",
    "                          'shuffle': True}\n",
    "            train_generator = CustomDataGen(**params_train)\n",
    "            val_generator = CustomDataGen(**params_val)\n",
    "            # Count how many images is in the dataset\n",
    "            num_train_imgs = len(os.listdir(os.path.join(DATA_DIR , TRAIN_FOLDER ,'x_train/train/')))\n",
    "            num_val_imgs = len(os.listdir(os.path.join(DATA_DIR , TRAIN_FOLDER , 'x_val/val/')))\n",
    "            # Define steps per epoch the model will perform. It is based on the batch_size.\n",
    "            steps_per_epoch = num_train_imgs // batch_size\n",
    "            steps_per_epoch_val = num_val_imgs // batch_size\n",
    "            if isVerboseOn:\n",
    "                print(\"num_train_imgs: \", num_train_imgs)\n",
    "                print(\"num_val_imgs: \", num_val_imgs)\n",
    "                print(\"steps_per_epoch: \", steps_per_epoch)\n",
    "                print(\"steps_per_epoch_val: \", steps_per_epoch_val)\n",
    "        else:\n",
    "            # LDR generator\n",
    "            train_image_generator, mask_image_generator, valid_img_generator, valid_mask_generator = getLDRGenerator(\n",
    "                DATA_DIR + TRAIN_FOLDER, batch_size)\n",
    "            train_generator = zip(train_image_generator, mask_image_generator)\n",
    "            val_generator = zip(valid_img_generator, valid_mask_generator)\n",
    "            # Count how many images is in the dataset\n",
    "            num_train_imgs = len(os.listdir(os.path.join(DATA_DIR , TRAIN_FOLDER , '/x_train/train')))\n",
    "            num_val_imgs = len(os.listdir(os.path.join(DATA_DIR , TRAIN_FOLDER , '/x_val/validation')))\n",
    "            # Define steps per epoch the model will perform. It is based on the batch_size.\n",
    "            steps_per_epoch_val = num_val_imgs // batch_size\n",
    "            steps_per_epoch = num_train_imgs // batch_size\n",
    "            if isVerboseOn:\n",
    "                print(\"num_train_imgs: \", num_train_imgs)\n",
    "                print(\"num_val_imgs: \", num_val_imgs)\n",
    "                print(\"steps_per_epoch: \", steps_per_epoch)\n",
    "                print(\"steps_per_epoch_val: \", steps_per_epoch_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c159f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "if isTrainingOn or isPredictionOn:\n",
    "    if OLD_MODEL_NAME:\n",
    "        # We want to train based on another model\n",
    "        modelLoaded = getOldModel(os.path.join(MODEL_FOLDER , OLD_MODEL_NAME))\n",
    "        if isVerboseOn:\n",
    "            print('Old model loaded')\n",
    "    else:\n",
    "        # We want to train based on given backbone\n",
    "        modelLoaded = getModel(BACKBONE)\n",
    "        if isVerboseOn:\n",
    "            print('Backbone ' + BACKBONE + ' loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "if isTrainingOn:\n",
    "    if useStandardDataset:\n",
    "        # Train without generator. Not recommended\n",
    "        modelTrained, history = trainModelWithoutGenerator(model=modelLoaded, data=[x_train, x_val, y_train, y_val],\n",
    "                                                           epochs=epochs, batch_size=batch_size,\n",
    "                                                           steps_per_epoch=steps_per_epoch,\n",
    "                                                           verbose=verbose, callbacks=callbacks)\n",
    "    else:\n",
    "        # Train with generator.\n",
    "        modelTrained, history = trainModelGenerator(model=modelLoaded, data=[train_generator, val_generator],\n",
    "                                                    epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "                                                    callbacks=callbacks)\n",
    "\n",
    "    # Save model\n",
    "    modelTrained.save('{}{}.h5'.format(MODEL_FOLDER, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isPredictionOn:\n",
    "    if isTrainingOn:\n",
    "        # We want to use the model that was just trained\n",
    "        modelLoaded = keras.models.load_model('{}/{}.h5'.format(MODEL_FOLDER, MODEL_NAME), compile=False)\n",
    "    if isHdr:\n",
    "        # HDR prediction\n",
    "        result = predictHdr(PREDICTION_PATH, PREDICTION_SAVE_FOLDER, patch_x, patch_y, stride, model = modelLoaded, verbose = isVerboseOn)\n",
    "    else:\n",
    "        # LDR prediction\n",
    "        result = predictLDR(PREDICTION_PATH, PREDICTION_SAVE_FOLDER, patch_x, patch_y, stride, model = modelLoaded, verbose=isVerboseOn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beac859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segModels39Kernel",
   "language": "python",
   "name": "segmodels39kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
